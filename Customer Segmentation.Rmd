---
title: "Online Retail Customer Analytics"
author: "Stone"
date: "11/21/2019"
output:
  pdf_document:
    toc: yes
  html_document:
    theme: darkly
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1: Background

Nowadays, marketing specialists are becoming more and more data-driven. Everyday they collect hundreds and thousands rows of data, data becomes one of the most invaluable properties for an organization. Knowing your customers well and segmentating them into different groups is one of the key business strategies that business organizations are deploying. 

In this analysis, I will provide a **Customer Segmentation** strategy based on RFM model for an online retailer based in UK, with some customers from some other places in Europe. 

I hope you enjoy reading this analysis :). 

```{r, message = FALSE}
library(tidyverse)
online_orig <- readxl::read_excel("/Volumes/GoogleDrive/My Drive/University of Notre Dame/MSBA Fall Semester/Module 2/Machine Learning/Code/Fred/E03 KMeansClustering/Python/Online Retail.xlsx")
online <- online_orig
```

Let's have a look at our data.
```{r}
glimpse(online)
summary(online)
```

As we can see, we have **8** variables and **541909** records. Now let's jump to EDA to get a better view of our data.

</br>

# Part 2: Exploratory Data Analysis

## 1. Missing Data & Outliers

```{r}
sort(colSums(is.na(online)), decreasing = TRUE)

```

Looks like we have a lot of missing customer IDs, we can then remove the rows with missing customerIDs.

```{r}
online <- online %>% 
  filter(!is.na(CustomerID) & !is.na(Description))
```

```{r}
summary(online)
```

I also noticed that **Quantity** has minimum value: *-80995*. Let's visualize the data.

```{r}
ggplot(data = online) +
  geom_point(aes(x = seq(Quantity), y = Quantity))
```

We can see we have some strange values (quantity less than 0), so I need to remove these negative values as well.

```{r}
online <- online %>% 
  filter(Quantity > 0)

ggplot(data = online) +
  geom_point(aes(x = seq(Quantity), y = Quantity))
```

```{r}
min(online$InvoiceDate)
max(online$InvoiceDate)
```

We can see the dataset is from **2010-12-01** to **2011-12-09**, a little bit over one year. To make our customer egmentation more consistent, we will consider their transaction behaviors in an one-year duration. 

```{r}
online <- online %>% 
  filter(InvoiceDate >= "2010-12-09")
min(online$InvoiceDate)
```

</br>

## 2. Countries
```{r, message = FALSE}
library(gridExtra)
library(scales)
online %>% 
    group_by(Country) %>% 
    summarize(n = n()) %>%
    ggplot(aes(x = reorder(Country, n), y = n)) +
    geom_histogram(stat = "identity", fill = "pink")+
    scale_y_continuous(breaks = seq(0,360000, by =50000), labels = comma)+
    coord_flip() +
    labs(title = "Nbr of Records with UK", x = "Country", y = "Number of Records")+
    theme_minimal()
  
online %>% 
    group_by(Country) %>% 
    filter(Country != "United Kingdom") %>% 
    summarize(n = n()) %>%
    ggplot(aes(x = reorder(Country, n), y = n)) +
    geom_histogram(stat = "identity", fill = "purple")+
    scale_y_continuous(breaks = seq(0,10000, by =1000), labels = comma)+
    coord_flip() +
    labs(title = "Nbr of Records without UK",x = "Country", y = "Number of Records")+
    theme_minimal()

```

```{r}
nbr_uk <- online %>% 
  filter(Country == "United Kingdom")

nrow(nbr_uk)/nrow(online)

```

From the two graphs above, we can see that most of the transactions happened in UK (accounts for 89% of the entire dataset). 

## 3. Customers

We have nearly 385150 rows of records in our dataset, but a question might pop up: how many customers do we have? Where are they?

```{r}
nbr_of_customers <- online %>% 
    group_by(CustomerID) %>% 
    summarise(n = n())
nrow(nbr_of_customers)
```

We have **4275** unique customers in our data. Let's see where are they?


```{r}
customer_with_country <- online %>% 
    dplyr::select(CustomerID, Country) %>% 
    distinct(CustomerID, Country)
    #duplicated(CustomerID)

nrow(customer_with_country)
```

Notice that we have 4283 customers now. One of the reasons why we have slightly more customers might be we have customers who *shopped* at different countries. let's test our hypothesis.

```{r}
customer_with_country[which(duplicated(customer_with_country$CustomerID)),]
customer_with_country[customer_with_country$CustomerID == "12370",]
```

CustomerID: 12370 shopped at both Cyprus and Austria, our hypothesis is correct! Now let's make a visualization to see the breakdown of countries by customers.

```{r}
customer_with_country %>% 
    group_by(Country) %>% 
    summarize(n = n()) %>%
    ggplot(aes(x = reorder(Country, n), y = n)) +
    geom_histogram(stat = "identity", fill = "lightblue")+
    scale_y_continuous(breaks = seq(0,4000, by =500), labels = comma)+
    coord_flip() +
    labs(title = "Number of Customers with UK", x = "Country", y = "Number of Customers")+
    theme_minimal()


customer_with_country %>% 
    group_by(Country) %>% 
    filter(Country != "United Kingdom") %>% 
    summarize(n = n()) %>%
    ggplot(aes(x = reorder(Country, n), y = n)) +
    geom_histogram(stat = "identity", fill = "orange")+
    scale_y_continuous(breaks = seq(0,4000, by =500), labels = comma)+
    coord_flip() +
    labs(title = "Number of Customers without UK", x = "Country", y = "Number of Customers")+
    theme_minimal()
```

Germany, France, Spain, Belgium and Swizerland are some of the largest markets in Europe as well. The marketing department in this online retailer should pay special attention on those countries if they try to find specific countries to enlarge the market size.

## 4. Products
I'm also curious about what do people usually buy through this online market.

```{r}
online %>% 
  dplyr::select(CustomerID, Description) %>% 
  group_by(Description) %>% 
  summarise(n = n()) %>%
  head(.,10) %>% 
  ggplot(aes(x = reorder(Description, -n), y = n)) +
  geom_histogram(stat = "identity") +
  theme(axis.text.x = element_text(angle=90, vjust=0.6))
  
```

## 5. Time

```{r, message = FALSE}
library(lubridate)
online %>% 
  dplyr::select(InvoiceDate) %>% 
  mutate(InvoiceDate = lubridate::date(InvoiceDate)) %>% 
  group_by(InvoiceDate) %>% 
  summarise(n = n()) %>% 
  head(.,20) %>% 
  ggplot(aes(x = reorder(InvoiceDate, n), y = n)) +
  geom_histogram(stat = "identity", fill = "gold")+
  scale_y_continuous(breaks = seq(0,5000, by =300), labels = comma)+
  labs(x = "date") +
  coord_flip()+
  theme_minimal()
```

There are some days in a year people really shopped a lot. It usually happened in December and January, as a lot of people started preparing for Christmas and New Year.

```{r}
online %>% 
  dplyr::select(InvoiceDate) %>% 
  mutate(InvoiceDate = lubridate::wday(InvoiceDate, label = TRUE, abbr = TRUE)) %>% 
  group_by(InvoiceDate) %>% 
  summarise(n = n()) %>% 
  ggplot(aes(x = InvoiceDate, y = n)) +
  geom_histogram(stat = "identity", fill = "orange")+
  scale_y_continuous(breaks = seq(0,70000, by =10000), labels = comma)+
  labs(x = "Day of Week") + 
  theme_minimal()

```

Thursday is a peak when people usually shopped a lot. There's no huge difference between the rest of the week, as the internet has been growing very fast, people can shop online whenever they want.

# Part 3: Clustering

## 1. Feature Engineering

`RFM` is a very powerful customer segmentation technique that uses past purchase behavior to divide customers into groups. Based on the **RFM** model, we need to calculate the following values to better segment our customers:

* **RECENCY** (R): Time since last purchase

* **FREQUENCY** (F): Total number of purchases

* **MONETARY VALUE** (M): Total monetary value

```{r}
head(online)
```

### 1.1 Recency

```{r, message = FALSE}

recency <- online %>% 
  dplyr::select(CustomerID, InvoiceDate) %>% 
  mutate(recency = as.Date("2011-12-09") - as.Date(InvoiceDate))

recency <- recency %>% 
  dplyr::select(CustomerID, recency) %>% 
  group_by(CustomerID) %>% 
  slice(which.min(recency))

head(recency)
```

### 1.2 Frequency

```{r}

nbr_item <- online %>%
    dplyr::select(CustomerID, InvoiceDate) %>% 
    group_by(CustomerID, InvoiceDate) %>% 
    summarize(n.item = n())
nbr_item  
   
frequency <- nbr_item %>% 
    dplyr::select(CustomerID) %>%
    group_by(CustomerID) %>% 
    summarize(customer.frequency = n())

head(frequency)

```

### 1.3 Monetary Value

```{r}
online <- online %>% 
  mutate(Value = Quantity * UnitPrice)

customer_value <- online %>% 
  group_by(CustomerID) %>% 
  summarize(monetary.value = sum(Value))

head(customer_value)

```

Now, we have finished our feature engineering process, we need to join three data franes together.

```{r}
frm <- recency %>% 
  dplyr::inner_join(., frequency, by = "CustomerID") %>% 
  dplyr::inner_join(., customer_value, by = "CustomerID")

frm <- frm %>% 
  mutate(recency = str_replace(recency, " days", "")) %>% 
  mutate(recency = as.numeric(recency)) %>% 
  ungroup()


```

### 1.5 Normalization

We don't need *CustomerID* as this moment, so we will delete this variable.
```{r}
model_data <- frm %>% 
  dplyr::select(-CustomerID)
```

```{r}
model_data %>% 
  gather() %>% 
  ggplot() +
  geom_histogram(mapping = aes(x = value, fill = key), color = "black") +
  facet_wrap(~ key, scales = "free")+
  theme_minimal()
```

I will employ **z-score** to do normalization here.
```{r}
model_data_z <- scale(model_data)
summary(model_data_z)
```

## 2. K-Means Clustering

### 2.1 Elbow Method
```{r, message = FALSE}
library(stats)
library(factoextra)

wcss <- vector()
n = 20
set.seed(1234)
for(k in 1:n) {
  wcss[k] <- sum(kmeans(model_data_z, k)$withinss)
}

as.data.frame(wcss)
```

Now let's visualize the values of WCSS as they relate to number of clusters

```{r}
tibble(value = wcss) %>% 
  ggplot(mapping = aes(x = seq(1, length(wcss)), y = value)) +
  geom_point() +
  geom_line() +
  labs(title = "The Elbow Method", y = "WCSS", x = "Number of Clusters (k)") +
  theme_minimal()
```

From **Elbow Method**, k could be 5, 8, 10 or 15. Let's visualize our clusters to get additional insight.

```{r}
set.seed(1234)
k_5 <- kmeans(model_data_z, centers = 5, nstart = 25)
k_8 <- kmeans(model_data_z, centers = 8, nstart = 25)
k_10 <- kmeans(model_data_z, center = 10, nstart = 25)
k_15 <- kmeans(model_data_z, center = 15, nstart = 25)

# Plot and compare the number of clusters affects the results.
p1 <- fviz_cluster(k_5, geom = "point", data = model_data_z) + ggtitle("k = 5")
p2 <- fviz_cluster(k_8, geom = "point", data = model_data_z) + ggtitle("k = 8")
p3 <- fviz_cluster(k_10, geom = "point", data = model_data_z) + ggtitle("k = 10")
p4 <- fviz_cluster(k_15, geom = "point", data = model_data_z) + ggtitle("k = 15")

grid.arrange(p1,p2,p3,p4, nrow = 2)

```

### 2.2 Select The Appropriate K

> **k = 5**

```{r}
as.data.frame(k_5$size)
```

> **k = 8**

```{r}
as.data.frame(k_8$size)
```

> **k = 10**

```{r}
as.data.frame(k_10$size)
```

> **k = 15**

```{r}
as.data.frame(k_15$size)
```

All of the clusters produced by different k seem to have some issues. A lot of clusters only have less than 10 customers inside the clusters, which becomes extremely difficult for the marketing team to do corresponding marketing promotions. 

One of my assumptions is that the problem could be related to how I normalized the data (I used Z-score normalization in the previous section). So I decided to use another common normalization method called: **Min-Max** Normalization. Let's give it a try. 

### 2.3 Improving Model Performance

```{r}
normalize <- function(x){
  return ((x - min(x)) / (max(x) - min(x)))
}
model_data_mm <- as.data.frame(map(model_data, normalize))
summary(model_data_mm)
```

I will use Elbow method again.

```{r}
library(stats)
library(factoextra)

wcss2 <- vector()
n = 20
set.seed(1234)
for(k in 1:n) {
  wcss2[k] <- sum(kmeans(model_data_mm, k)$withinss)
}

as.data.frame(wcss2)

```


```{r}
tibble(value = wcss2) %>% 
  ggplot(mapping = aes(x = seq(1, length(wcss2)), y = value)) +
  geom_point() +
  geom_line() +
  labs(title = "The Second Elbow Method", y = "WCSS", x = "Number of Clusters (k)") +
  theme_minimal()
```

This time, I will just invite 5,6,7 and 8 to be our potential K candidates. Let's visualize them.

```{r}
set.seed(1234)
k_5 <- kmeans(model_data_mm, centers = 5, nstart = 25)
k_6 <- kmeans(model_data_mm, centers = 6, nstart = 25)
k_7 <- kmeans(model_data_mm, centers = 7, nstart = 25)
k_8 <- kmeans(model_data_mm, center = 8, nstart = 25)

# Plot and compare the number of clusters affects the results.
p1 <- fviz_cluster(k_5, geom = "point", data = model_data_mm) + ggtitle("k = 5")
p2 <- fviz_cluster(k_6, geom = "point", data = model_data_mm) + ggtitle("k = 6")
p3 <- fviz_cluster(k_7, geom = "point", data = model_data_mm) + ggtitle("k = 7")
p4 <- fviz_cluster(k_8, geom = "point", data = model_data_mm) + ggtitle("k = 8")

grid.arrange(p1,p2,p3,p4, nrow = 2)
```

> **k=5**

```{r}
as.data.frame(k_5$size)
```

> **k=6**

```{r}
as.data.frame(k_6$size)
```

> **k=7**

```{r}
as.data.frame(k_7$size)
```

> **k=8**

```{r}
as.data.frame(k_8$size)
```

We can see, after *min-max normalization*, our model does perform a bit better. Now, I will assign the cluster to each customers.

> *when k is 5*

```{r}
frm_k5 <- frm %>% 
  mutate(cluster = k_5$cluster)

head(frm_k5)

frm_k5 %>%
  group_by(cluster) %>% 
  summarize(number = n(),
            frequency = round(mean(customer.frequency)),
            recency = round(mean(recency)),
            monetary = round(mean(monetary.value)))
  
```

</br>

> *when k is 6*

```{r}
frm_k6 <- frm %>% 
  mutate(cluster = k_6$cluster)

head(frm_k6)

frm_k6 %>%
  group_by(cluster) %>% 
  summarize(number = n(),
            frequency = round(mean(customer.frequency)),
            recency = round(mean(recency)),
            monetary = round(mean(monetary.value)))
```

</br>

> *when k is 7*

```{r}
frm_k7 <- frm %>% 
  mutate(cluster = k_7$cluster)
head(frm_k7)

frm_k7 %>%
  group_by(cluster) %>% 
  summarize(number = n(),
            frequency = round(mean(customer.frequency)),
            recency = round(mean(recency)),
            monetary = round(mean(monetary.value)))
```

</br>

> *when k=8*

```{r}
frm_k8 <- frm %>% 
  mutate(cluster = k_8$cluster)
head(frm_k8)

frm_k8 %>%
  group_by(cluster) %>% 
  summarize(number = n(),
            frequency = round(mean(customer.frequency)),
            recency = round(mean(recency)),
            monetary = round(mean(monetary.value)))
```

</br>

From the summary stats of four different ks above. It seems that when k equals to 6, we have some more demonstrable clusters.

```{r}
frm_k6 %>%
  group_by(cluster) %>% 
  summarize(number = n(),
            frequency = round(mean(customer.frequency)),
            recency = round(mean(recency)),
            monetary = round(mean(monetary.value)))
```

* For customers (24) in **cluster 1**, they are our **VIP** customers. They generated a large monetary value (85239), they shopped 67 times a year, and their last day of purchase was also very close to the end of the dataset. These group of customers should have very high demand, high income, so need to treate them very well in order to retain them.

* For customers (499) in **cluster 2**, they are our **almost lost** customers. We see they did still buy something twice a year, and their last purchase day was within a tolerable month range. Marketing department should do something instantly to attract them to shop again. 

* For customers (413) in **cluster 3**, they are our **lost** customers. They haven't shopped in our store for at least half year, but they might be re-activated if the marketing team has any corresponding strategies to improve this situation.

* For customers (1052) in **cluster 4**, they are our **normal** customers. It is possible to convert this group to be our important customers.

* For customers (306) in **cluster 5**, they are our **least important** customers. It's been almost a year since their last purchase, it might be hard to get them back.

* For customers (1981) in **cluster 6**, they are our **important** customers. They produced the second largest monetary value among six groups, and the customers in this group shopped frequently as well.

# Part 4: Final thought

**Customer Segmentation** is an important marketing strategy that organizations should deploy to better understand the market and make actionable decisions to boost their sales. K-Means clustering is a simple but powerful machine learning algorithm for organizations to implement. 

One final thing we need to do is to keep the RFM customer segmentation updated in order to improve our marketing performance.